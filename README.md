# ğŸ“ RAG AI Teaching Assistant (Fully Local)

A fully local, end-to-end Retrieval-Augmented Generation (RAG) pipeline designed as an AI teaching assistant for the Sigma Web Development course.

Students submit natural language questions.  
The system retrieves relevant lecture transcript segments and generates a grounded answer citing:

- ğŸ“º Video number
- â± Timestamp range

Runs 100% offline â€” no cloud APIs required.

---

## ğŸ§  Executive Summary

This project demonstrates a practical, modular RAG architecture built for:

- Resource-constrained environments
- Fully offline operation
- Privacy-focused AI systems
- Educational use cases

It integrates:

- Video processing
- Whisper-based speech recognition
- Dense vector retrieval (BGE-M3)
- Cosine similarity search
- Local LLM inference (Llama 3.2 via Ollama)

---

## ğŸ— Pipeline Architecture

Videos (.mp4)
      â†“
video_to_mp3.py
      â†“
MP3 audio files
      â†“
mp3_to_jsons.py (Whisper + Semantic Chunking)
      â†“
Chunked JSON transcripts
      â†“
pre_processesed_json.py (BGE-M3 Embeddings)
      â†“
embeddings.joblib (Vector Store)
      â†“
process_incoming.py
      â†“
Top-K Retrieval + Llama 3.2
      â†“
Timestamp-Grounded Answer

---

## ğŸ“ Project Structure

RAG-based-ai/
â”œâ”€â”€ Videos/                  # 15 raw lecture MP4 files  
â”œâ”€â”€ audios/                  # Converted MP3 files  
â”œâ”€â”€ jsons/                   # Whisper-generated transcripts  
â”œâ”€â”€ video_to_mp3.py          # Stage 1: Video â†’ Audio  
â”œâ”€â”€ mp3_to_jsons.py          # Stage 2: Audio â†’ Chunked JSON  
â”œâ”€â”€ pre_processesed_json.py  # Stage 3: JSON â†’ Embeddings  
â”œâ”€â”€ process_incoming.py      # Stage 4: Query â†’ Retrieval â†’ Answer  
â”œâ”€â”€ embeddings.joblib        # Serialized embedding store  
â”œâ”€â”€ embeddings.csv           # CSV export (debug)  
â”œâ”€â”€ prompt.txt               # Last prompt (debug)  
â”œâ”€â”€ response.txt             # Last response (debug)  
â”œâ”€â”€ README.md  
â””â”€â”€ .gitignore  


## ğŸ” Stage Breakdown

### 1ï¸âƒ£ Video â†’ Audio
File: video_to_mp3.py  
- Converts .mp4 files to .mp3  
- Uses ffmpeg  
- Ensures correct numeric sorting (vid1â€“vid15)  

---

### 2ï¸âƒ£ Audio â†’ Transcript + Semantic Chunking
File: mp3_to_jsons.py  

- Whisper model: small (CPU mode)  
- Language: Hindi â†’ Translated to English  
- Timestamp-aware transcription  
- Window-based chunk merging (3 segments per chunk)  

Why chunk merging?

Raw Whisper segments are very short (1â€“3 seconds).  
The system merges every 3 segments into one semantic chunk.

This improves:
- Semantic coherence  
- Embedding quality  
- Retrieval accuracy  

---

### 3ï¸âƒ£ Transcript â†’ Embeddings
File: pre_processesed_json.py  

- Embedding model: BGE-M3  
- Served locally via Ollama  
- Stored in Pandas DataFrame  
- Serialized using joblib  

Each row contains:
- Video number  
- Title  
- Start time  
- End time  
- Text  
- Embedding vector  

---

### 4ï¸âƒ£ Query â†’ Retrieval â†’ Grounded Response
File: process_incoming.py  

Steps:
1. User submits question  
2. Question is embedded  
3. Cosine similarity retrieves top 5 chunks  
4. Structured prompt sent to Llama 3.2  
5. Model generates timestamp-grounded answer  

Example output:

"The exercise is mentioned in video 13 at 481.04 â€“ 482.04 seconds. Please watch that segment."

---

## ğŸ§° Technology Stack

Video Processing: ffmpeg  
Transcription: Whisper (small)  
Embeddings: BGE-M3  
LLM: Llama 3.2  
Runtime: Ollama (localhost)  
Retrieval: sklearn cosine_similarity  
Storage: Pandas + joblib  
Language: Python 3.x  

---

## ğŸ’ª Strengths

- Fully offline & private  
- Modular 4-stage pipeline  
- Smart semantic chunking  
- Timestamp-grounded responses  
- Multilingual support (Hindi â†’ English)  
- Debug-friendly (prompt & response saved)  
- Clean separation of responsibilities  

## ğŸ¯ What This Project Demonstrates

- Practical RAG implementation  
- Local LLM deployment  
- Applied NLP  
- Embedding-based retrieval  
- System design thinking  
- Privacy-focused AI architecture  
- Resource-aware ML engineering  
